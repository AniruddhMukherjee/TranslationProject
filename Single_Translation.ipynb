{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10020489,
          "sourceType": "datasetVersion",
          "datasetId": 6170221
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AniruddhMukherjee/MultiModel_Translation_Project/blob/main/Single_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "CY-eVPy6ksjT"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "aniruddhmukherjee_translation_dataset_path = kagglehub.dataset_download('aniruddhmukherjee/translation-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "l5CqdpnUksjY"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:02:45.200036Z",
          "iopub.execute_input": "2024-11-26T15:02:45.200946Z",
          "iopub.status.idle": "2024-11-26T15:02:45.206795Z",
          "shell.execute_reply.started": "2024-11-26T15:02:45.200914Z",
          "shell.execute_reply": "2024-11-26T15:02:45.205929Z"
        },
        "id": "nmTp3Z0qksja",
        "outputId": "55477420-91de-4a05-9a1d-481a13b1631a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/translation-dataset/hin.txt\n/kaggle/input/translation-dataset/mar.txt\n/kaggle/input/translation-dataset/deu.txt\n/kaggle/input/translation-dataset/ben.txt\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Concatenate\n",
        "from tqdm import tqdm\n",
        "import re"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:02:52.842502Z",
          "iopub.execute_input": "2024-11-26T15:02:52.843325Z",
          "iopub.status.idle": "2024-11-26T15:03:03.797469Z",
          "shell.execute_reply.started": "2024-11-26T15:02:52.843294Z",
          "shell.execute_reply": "2024-11-26T15:03:03.796676Z"
        },
        "id": "4OnJhAsIksje"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:03:03.799181Z",
          "iopub.execute_input": "2024-11-26T15:03:03.799825Z",
          "iopub.status.idle": "2024-11-26T15:03:03.804906Z",
          "shell.execute_reply.started": "2024-11-26T15:03:03.799784Z",
          "shell.execute_reply": "2024-11-26T15:03:03.803857Z"
        },
        "id": "lba19x2Hksjf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess datasets\n",
        "def load_and_preprocess_datasets(filepaths):\n",
        "    combined_data = []\n",
        "    for lang, filepath in filepaths.items():\n",
        "        data = pd.read_csv(filepath, sep='\\t', header=None, names=[\"source\", \"target\", \"metadata\"])\n",
        "        data[\"source\"] = data[\"source\"].apply(clean_text)\n",
        "        data[\"target\"] = data[\"target\"].apply(clean_text).apply(lambda x: f'<{lang}> <START> ' + x + ' <END>')\n",
        "        combined_data.append(data[[\"source\", \"target\"]])\n",
        "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
        "    return combined_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:03:03.806334Z",
          "iopub.execute_input": "2024-11-26T15:03:03.806704Z",
          "iopub.status.idle": "2024-11-26T15:03:03.840274Z",
          "shell.execute_reply.started": "2024-11-26T15:03:03.806665Z",
          "shell.execute_reply": "2024-11-26T15:03:03.839597Z"
        },
        "id": "9I6KDMRgksjg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "def tokenize_and_pad(data, max_len):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(data)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "    pad_sequences_ = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "    return tokenizer, vocab_size, pad_sequences_\n",
        "\n",
        "# Filepaths for datasets\n",
        "filepaths = {\n",
        "    \"hindi\": \"/kaggle/input/translation-dataset/hin.txt\",\n",
        "    \"bengali\": \"/kaggle/input/translation-dataset/ben.txt\",\n",
        "    \"marathi\": \"/kaggle/input/translation-dataset/mar.txt\",\n",
        "    \"german\": \"/kaggle/input/translation-dataset/deu.txt\"\n",
        "}\n",
        "\n",
        "# Load and preprocess all datasets\n",
        "max_len = 40\n",
        "print(\"Loading and preprocessing datasets...\")\n",
        "combined_df = load_and_preprocess_datasets(filepaths)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "print(\"Tokenizing and padding sequences...\")\n",
        "source_tokenizer, source_vocab_size, source_padded = tokenize_and_pad(combined_df[\"source\"], max_len)\n",
        "target_tokenizer, target_vocab_size, target_padded = tokenize_and_pad(combined_df[\"target\"], max_len)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:03:03.84194Z",
          "iopub.execute_input": "2024-11-26T15:03:03.842217Z",
          "iopub.status.idle": "2024-11-26T15:03:22.718901Z",
          "shell.execute_reply.started": "2024-11-26T15:03:03.842191Z",
          "shell.execute_reply": "2024-11-26T15:03:22.71794Z"
        },
        "id": "37Js0PZpksjh",
        "outputId": "c934566f-8a27-4f84-92ea-743851c2d3a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading and preprocessing datasets...\nTokenizing and padding sequences...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the attention layer\n",
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=(input_shape[0][2], input_shape[0][2]),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=(input_shape[1][2], input_shape[0][2]),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=(input_shape[0][2], 1),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            encoder_full_seq = states[-1]\n",
        "            W_a_dot_s = tf.keras.backend.dot(encoder_full_seq, self.W_a)\n",
        "            U_a_dot_h = tf.keras.backend.expand_dims(tf.keras.backend.dot(inputs, self.U_a), 1)\n",
        "            e_i = tf.keras.backend.softmax(\n",
        "                tf.keras.backend.squeeze(tf.keras.backend.dot(tf.keras.backend.tanh(W_a_dot_s + U_a_dot_h), self.V_a),\n",
        "                                         axis=-1))\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            encoder_full_seq = states[-1]\n",
        "            c_i = tf.keras.backend.sum(encoder_full_seq * tf.keras.backend.expand_dims(inputs, -1), axis=1)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = tf.keras.backend.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = tf.keras.backend.sum(encoder_out_seq, axis=2)\n",
        "        _, e_outputs, _ = tf.keras.backend.rnn(energy_step, decoder_out_seq, [fake_state_e], constants=[encoder_out_seq])\n",
        "        _, c_outputs, _ = tf.keras.backend.rnn(context_step, e_outputs, [fake_state_c], constants=[encoder_out_seq])\n",
        "\n",
        "        return c_outputs, e_outputs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:03:22.720098Z",
          "iopub.execute_input": "2024-11-26T15:03:22.720377Z",
          "iopub.status.idle": "2024-11-26T15:03:22.730068Z",
          "shell.execute_reply.started": "2024-11-26T15:03:22.720351Z",
          "shell.execute_reply": "2024-11-26T15:03:22.729036Z"
        },
        "id": "yPMKiE0zksjj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the multi-language model\n",
        "def build_multi_language_model(source_vocab_size, target_vocab_size):\n",
        "    encoder_inputs = Input(shape=(max_len,))\n",
        "    encoder_emb = Embedding(source_vocab_size, 100, trainable=True)(encoder_inputs)\n",
        "\n",
        "    encoder_lstm = LSTM(300, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_emb)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_emb = Embedding(target_vocab_size, 100, trainable=True)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(300, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_emb, initial_state=encoder_states)\n",
        "\n",
        "    attn_layer = AttentionLayer()\n",
        "    attn_outputs, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "    decoder_concat_outputs = Concatenate(axis=-1)([decoder_outputs, attn_outputs])\n",
        "\n",
        "    decoder_dense = TimeDistributed(Dense(target_vocab_size, activation='softmax'))\n",
        "    decoder_outputs = decoder_dense(decoder_concat_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:03:34.628485Z",
          "iopub.execute_input": "2024-11-26T15:03:34.629372Z",
          "iopub.status.idle": "2024-11-26T15:03:34.635626Z",
          "shell.execute_reply.started": "2024-11-26T15:03:34.629338Z",
          "shell.execute_reply": "2024-11-26T15:03:34.634719Z"
        },
        "id": "KYLe8_1Wksjk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train the model\n",
        "print(\"Building the multi-language model...\")\n",
        "model = build_multi_language_model(source_vocab_size, target_vocab_size)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "print(\"Training the model...\")\n",
        "model.fit([source_padded, target_padded[:, :-1]], target_padded[:, 1:],\n",
        "          batch_size=64, epochs=5, validation_split=0.1)\n",
        "\n",
        "# Save the model\n",
        "model.save('multi_language_translator.h5')\n",
        "print(\"Model saved as 'multi_language_translator.h5'\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T15:03:36.166162Z",
          "iopub.execute_input": "2024-11-26T15:03:36.166944Z",
          "iopub.status.idle": "2024-11-26T17:36:31.580484Z",
          "shell.execute_reply.started": "2024-11-26T15:03:36.166896Z",
          "shell.execute_reply": "2024-11-26T17:36:31.579686Z"
        },
        "id": "4kp-3Sq9ksjk",
        "outputId": "30540c3c-92b9-4814-dee0-9b05deba3921"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Building the multi-language model...\nTraining the model...\nEpoch 1/5\n\u001b[1m4704/4704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1845s\u001b[0m 390ms/step - loss: 1.0394 - val_loss: 1.2638\nEpoch 2/5\n\u001b[1m4704/4704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1837s\u001b[0m 391ms/step - loss: 0.4197 - val_loss: 1.0114\nEpoch 3/5\n\u001b[1m4704/4704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1836s\u001b[0m 390ms/step - loss: 0.2758 - val_loss: 0.9387\nEpoch 4/5\n\u001b[1m4704/4704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1835s\u001b[0m 390ms/step - loss: 0.2191 - val_loss: 0.9147\nEpoch 5/5\n\u001b[1m4704/4704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1819s\u001b[0m 387ms/step - loss: 0.1881 - val_loss: 0.9086\nModel saved as 'multi_language_translator.h5'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "htCDEyOVksjl"
      }
    }
  ]
}